{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","authorship_tag":"ABX9TyP5A11FTa7TW48Bj9GDxmN3"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","source":["!pip install -q --upgrade keras-cv\n","!pip install -q --upgrade k"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"qyXaD0mWlOGU","executionInfo":{"status":"ok","timestamp":1718014325405,"user_tz":-420,"elapsed":14075,"user":{"displayName":"Aditia Dika Putra Laksamana","userId":"16933478281462837845"}},"outputId":"eed50193-1744-484d-f4c8-209efcc835a6"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/650.7 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m645.1/650.7 kB\u001b[0m \u001b[31m21.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m650.7/650.7 kB\u001b[0m \u001b[31m15.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/950.8 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m950.8/950.8 kB\u001b[0m \u001b[31m61.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Building wheel for k (setup.py) ... \u001b[?25l\u001b[?25hdone\n"]}]},{"cell_type":"code","execution_count":null,"metadata":{"id":"uDmWEC-2cNjs"},"outputs":[],"source":["# Import libraries\n","import tensorflow as tf\n","from keras_cv.models import segmentation as sm\n","from tensorflow.keras.preprocessing.image import img_to_array, load_img\n","import numpy as np\n","from google.colab import drive\n","# from pathlib import Path\n","\n","drive.mount('/content/drive', force_remount=True)"]},{"cell_type":"code","source":["# Define paths\n","# base_dir = Path('/content/drive/My Drive/Dataset/dataset_ta/')  # Mounted Drive path (optional)\n","# base_dir = Path('path/to/your/local/dataset')  # Local dataset path\n","train_dir = '/content/drive/My Drive/Dataset/dataset_ta/Dataset Mapir/per-tanaman/'\n","val_dir = '/content/drive/My Drive/Dataset/dataset_ta/MAPIR_TEST_IMG/Coba di Lokasi'\n","# test_dir = base_dir / 'test'"],"metadata":{"id":"7LB7SiFcixtj"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# hyperparams\n","img_height = 224\n","img_width = 224\n","channel = 3\n","epoch = 100\n","batch = 16"],"metadata":{"id":"PTBLN7KVce6y"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from tensorflow.keras.preprocessing.image import load_img\n","\n","# Replace 'path/to/your/image.jpg' with the actual path to your image\n","img = load_img('/content/drive/My Drive/Dataset/dataset_ta/Dataset Mapir/per-tanaman/2024_0608/2024_0608_111357_002.JPG')\n","\n","if img is not None:\n","    print(\"Image loaded successfully!\")\n","else:\n","    print(\"Error loading image. Check path and format.\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5GxXXc5BybiJ","executionInfo":{"status":"ok","timestamp":1718019044900,"user_tz":-420,"elapsed":591,"user":{"displayName":"Aditia Dika Putra Laksamana","userId":"16933478281462837845"}},"outputId":"7374a2df-c288-48eb-901e-7532493d1039"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Image loaded successfully!\n"]}]},{"cell_type":"code","source":["from tensorflow.keras.preprocessing.image import ImageDataGenerator\n","\n","# Data augmentation (optional)\n","train_datagen = ImageDataGenerator(\n","    rotation_range=20,\n","    width_shift_range=0.2,\n","    height_shift_range=0.2,\n","    shear_range=0.2,\n","    zoom_range=0.2,\n","    horizontal_flip=True,\n","    fill_mode=\"nearest\"\n",")"],"metadata":{"id":"yuyRIkKCcjBP"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Define validation data generator (no augmentation)\n","val_datagen = ImageDataGenerator()\n","\n","# Create data generators\n","train_generator = train_datagen.flow_from_directory(\n","    train_dir,\n","    target_size=(img_height, img_width),\n","    batch_size=batch,\n","    class_mode=\"binary\"\n",")\n","val_generator = val_datagen.flow_from_directory(\n","    val_dir,\n","    target_size=(img_height, img_width),\n","    batch_size=batch,\n","    class_mode=\"binary\"\n",")"],"metadata":{"id":"Iq9A1FAucm71","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1718019132962,"user_tz":-420,"elapsed":910,"user":{"displayName":"Aditia Dika Putra Laksamana","userId":"16933478281462837845"}},"outputId":"82454483-6589-4388-8f05-317c48ca8dff"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Found 235 images belonging to 1 classes.\n","Found 5 images belonging to 1 classes.\n"]}]},{"cell_type":"code","source":["print(len(train_generator))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5kl19D3ty8Y9","executionInfo":{"status":"ok","timestamp":1718017060554,"user_tz":-420,"elapsed":537,"user":{"displayName":"Aditia Dika Putra Laksamana","userId":"16933478281462837845"}},"outputId":"67ff30c2-cd77-4b04-d504-cd5e5bbb9a6c"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["0\n"]}]},{"cell_type":"code","source":["import keras_cv\n","backbone = keras_cv.models.ResNet50V2Backbone(input_shape=[img_height, img_width, channel])\n","\n","# Load pre-trained model (modify as needed)\n","model = sm.DeepLabV3Plus(backbone,num_classes=1)\n","# +1 to account for the polybag segmentation class\n","\n","# Compile the model\n","model.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n","\n","# Train the model\n","model.fit(\n","    train_generator,\n","    epochs=epoch,\n","    validation_data=val_generator,\n",")\n","\n","# Save the trained model (replace with your desired name)\n","# model.save(\"plant_segmentation_model.h5\")\n","\n","print(\"Segmentation model training complete!\")"],"metadata":{"id":"M2q4i63-cpAb","colab":{"base_uri":"https://localhost:8080/","height":773},"executionInfo":{"status":"error","timestamp":1718019778631,"user_tz":-420,"elapsed":6874,"user":{"displayName":"Aditia Dika Putra Laksamana","userId":"16933478281462837845"}},"outputId":"6fc9bd5e-95cc-4604-81e8-b258ce05ca2d"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/50\n"]},{"output_type":"error","ename":"ValueError","evalue":"in user code:\n\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\", line 1401, in train_function  *\n        return step_function(self, iterator)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\", line 1384, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\", line 1373, in run_step  **\n        outputs = model.train_step(data)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\", line 1151, in train_step\n        loss = self.compute_loss(x, y, y_pred, sample_weight)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\", line 1209, in compute_loss\n        return self.compiled_loss(\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/compile_utils.py\", line 277, in __call__\n        loss_value = loss_obj(y_t, y_p, sample_weight=sw)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/losses.py\", line 143, in __call__\n        losses = call_fn(y_true, y_pred)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/losses.py\", line 270, in call  **\n        return ag_fn(y_true, y_pred, **self._fn_kwargs)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/losses.py\", line 2532, in binary_crossentropy\n        backend.binary_crossentropy(y_true, y_pred, from_logits=from_logits),\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/backend.py\", line 5822, in binary_crossentropy\n        return tf.nn.sigmoid_cross_entropy_with_logits(\n\n    ValueError: `logits` and `labels` must have the same shape, received ((None, None, None, 1) vs (None,)).\n","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m<ipython-input-78-b111f2e53563>\u001b[0m in \u001b[0;36m<cell line: 12>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;31m# Train the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m model.fit(\n\u001b[0m\u001b[1;32m     13\u001b[0m     \u001b[0mtrain_generator\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     68\u001b[0m             \u001b[0;31m# To get the full stack trace, call:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m             \u001b[0;31m# `tf.debugging.disable_traceback_filtering()`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m             \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\u001b[0m in \u001b[0;36mtf__train_function\u001b[0;34m(iterator)\u001b[0m\n\u001b[1;32m     13\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m                     \u001b[0mdo_return\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m                     \u001b[0mretval_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconverted_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep_function\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfscope\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m                 \u001b[0;32mexcept\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m                     \u001b[0mdo_return\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mValueError\u001b[0m: in user code:\n\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\", line 1401, in train_function  *\n        return step_function(self, iterator)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\", line 1384, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\", line 1373, in run_step  **\n        outputs = model.train_step(data)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\", line 1151, in train_step\n        loss = self.compute_loss(x, y, y_pred, sample_weight)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\", line 1209, in compute_loss\n        return self.compiled_loss(\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/compile_utils.py\", line 277, in __call__\n        loss_value = loss_obj(y_t, y_p, sample_weight=sw)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/losses.py\", line 143, in __call__\n        losses = call_fn(y_true, y_pred)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/losses.py\", line 270, in call  **\n        return ag_fn(y_true, y_pred, **self._fn_kwargs)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/losses.py\", line 2532, in binary_crossentropy\n        backend.binary_crossentropy(y_true, y_pred, from_logits=from_logits),\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/backend.py\", line 5822, in binary_crossentropy\n        return tf.nn.sigmoid_cross_entropy_with_logits(\n\n    ValueError: `logits` and `labels` must have the same shape, received ((None, None, None, 1) vs (None,)).\n"]}]},{"cell_type":"code","source":["# Function to segment and classify images\n","def segment_and_classify(image_path, plant_id):\n","    # Load image\n","    img = load_image(image_path, (img_height, img_width))\n","\n","    # Preprocess image (if needed)\n","    img = preprocess_image(img)  # Define your preprocessing function\n","\n","    # Predict mask and classification\n","    predictions = model.predict(np.expand_dims(img, axis=0))\n","    mask = predictions[0, :, :, 0] > 0.5  # Threshold for polybag segmentation\n","\n","    # Extract class index and convert to plant health label\n","    class_index = np.argmax(predictions[1, 0])\n","    plant_health = plant_health_map.get(str(class_index), \"unknown\")\n","\n","    # Generate output filename based on plant ID and health\n","    output_filename = f\"{output_dir}/{plant_id}_{plant_health}.jpg\"\n","\n","    # Apply mask to original image (optional)\n","    segmented_img = apply_mask(img, mask)  # Define your mask application function\n","\n","    # Save segmented image\n","    save_image(segmented_img, output_filename)\n","\n","# Loop through test images and segment based on plant ID\n","for image_path in test_dir.glob(\"*.jpg\"):\n","    plant_id = image_path.stem[:3]  # Assuming plant ID is in the filename prefix\n","    segment_and_classify(image_path, plant_id)\n","\n","# Define helper functions (load_image, preprocess_image, apply_mask, save_image)\n","# Implement these functions based on your specific image format and library\n","\n","print(\"Segmentation and classification complete. Check the output directory for results.\")"],"metadata":{"id":"Rk5YvFLRcsMO"},"execution_count":null,"outputs":[]}]}